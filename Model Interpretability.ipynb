{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Motor vehicle crashes remain a significant public safety concern in many cities across the globe. Understanding the underlying causes of traffic accidents is essential for improving road safety, optimizing traffic regulations, and informing policy decisions. With the increasing availability of open data, machine learning offers powerful tools for discovering patterns and predicting outcomes that can lead to actionable insights.\n",
    "\n",
    "This project uses publicly available crash, vehicle, and people datasets from the City of Chicago to develop a predictive model. The goal is to analyze a wide range of factors — including vehicle information, road conditions, and driver characteristics — to identify the primary contributory cause of each accident. By modeling this problem, we aim to support traffic safety stakeholders in better understanding crash dynamics and designing targeted interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "Motor vehicle accidents remain a persistent and costly public safety issue in urban centers. Every year, thousands of traffic accidents result in injuries, fatalities, and significant economic losses. Understanding the primary contributory causes of these crashes can help policymakers, urban planners, and safety boards implement targeted interventions to reduce accidents.\n",
    "\n",
    "#### Objectives\n",
    "Build a model that can predict the primary contributory cause of a car accident, given information about the car, the people in the car, the road conditions etc.\n",
    "\n",
    "#### Key Questions\n",
    "1. What are the most significant factors contributing to vehicle crashes in Chicago?\n",
    "2. How do weather and lighting conditions impact accident causation patterns?\n",
    "3. How do driver demographics (age, gender) correlate with accident causes?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "This project uses three datasets published by City of Chicago (https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if)\n",
    "\n",
    "* Traffic Crashes-Crashes Data: Shows information about each traffic crash on city streets within the City of Chicago\n",
    "* Traffic Crashes-Driver/Passenger Data: Contains information about people involved in a crash and if any injuries were sustained\n",
    "* Traffic Crashes- Vehicle Data: Contains information about vehicles involved in a traffic crash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load crashes dataset\n",
    "crashes_df=pd.read_csv('.\\DATA\\Crashes.csv', low_memory=False)\n",
    "crashes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Vehicle dataset\n",
    "Vehicle_df=pd.read_csv('.\\DATA\\Vehicles.csv', low_memory=False)\n",
    "Vehicle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load People dataset\n",
    "People_df=pd.read_csv('.\\DATA\\People.csv', low_memory=False)\n",
    "People_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets\n",
    "people_vehicle_df=pd.merge(People_df,Vehicle_df, how='inner', on='CRASH_RECORD_ID')\n",
    "\n",
    "data_df=pd.merge(crashes_df, people_vehicle_df,  how='inner', on='CRASH_RECORD_ID')\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample of the data\n",
    "sample_df = data_df.sample(frac=0.2, random_state=42)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the data\n",
    "sample_df=sample_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to use\n",
    "cols_to_keep = [\n",
    "        'CRASH_DATE', 'POSTED_SPEED_LIMIT', 'WEATHER_CONDITION',\n",
    "        'LIGHTING_CONDITION', 'ROADWAY_SURFACE_COND', 'CRASH_TYPE',\n",
    "        'DAMAGE', 'PRIM_CONTRIBUTORY_CAUSE', 'SEX', 'AGE', 'INJURIES_TOTAL',\n",
    "        'VEHICLE_TYPE', 'MAKE', 'MODEL', 'VEHICLE_YEAR'\n",
    "    ]\n",
    "    \n",
    "data_df = sample_df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape of the data\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive statistics\n",
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "data_df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values with median(Numeric columns)\n",
    "data_df['AGE'].fillna(data_df['AGE'].median(), inplace=True)\n",
    "data_df['VEHICLE_YEAR'].fillna(data_df['VEHICLE_YEAR'].median(), inplace=True)\n",
    "\n",
    "# Categorical columns\n",
    "cat_cols = data_df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Fill in missing values with the most frequent value\n",
    "for col in cat_cols:\n",
    "    if data_df[col].isnull().any():  \n",
    "        most_frequent = data_df[col].mode(dropna=True)[0] \n",
    "        data_df[col].fillna(most_frequent, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "data_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "data_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the number of unique values in each column\n",
    "for col in cat_cols:\n",
    "    print(f\"{col}: {data_df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categoricals with too many categories to 'OTHER'\n",
    "for col in ['MAKE', 'MODEL', 'VEHICLE_TYPE']:\n",
    "        top_values = data_df[col].value_counts().nlargest(10).index\n",
    "        data_df[col] = data_df[col].where(data_df[col].isin(top_values), 'OTHER')\n",
    "    \n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are the most significant factors contributing to vehicle crashes in Chicago?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 5 most common causes\n",
    "top_causes = data_df['PRIM_CONTRIBUTORY_CAUSE'].value_counts().nlargest(5).index\n",
    "top_causes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent contributory cause was 'Unable to determine,' but this category was excluded from the analysis due to its lack of actionable insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with vague/non-actionable contributory causes to improve data quality\n",
    "data_df = data_df[~data_df['PRIM_CONTRIBUTORY_CAUSE'].isin([\n",
    "    'UNABLE TO DETERMINE',  # Excluded due to ambiguity\n",
    "    'NOT APPLICABLE'        # Excluded as it provides no causal insight\n",
    "])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 5 top causes\n",
    "top_causes = data_df['PRIM_CONTRIBUTORY_CAUSE'].value_counts().nlargest(5)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Bar plot\n",
    "sns.barplot(x=top_causes.values, y=top_causes.index, palette='viridis')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Top 5 Primary Contributory Causes of Crashes')\n",
    "plt.xlabel('Number of Crashes')\n",
    "plt.ylabel('Cause')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do weather conditions impact the likelihood of different types of accidents?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 weather conditions \n",
    "top_weather = data_df['WEATHER_CONDITION'].value_counts().nlargest(5).index\n",
    "\n",
    "# Top 5 contributory causes \n",
    "top_causes = data_df['PRIM_CONTRIBUTORY_CAUSE'].value_counts().nlargest(5).reset_index()\n",
    "top_cause_names = top_causes['index'].tolist() \n",
    "\n",
    "# Filter the data\n",
    "plot_data = data_df[\n",
    "    (data_df['WEATHER_CONDITION'].isin(top_weather)) & \n",
    "    (data_df['PRIM_CONTRIBUTORY_CAUSE'].isin(top_cause_names))\n",
    "]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.countplot(\n",
    "    x='WEATHER_CONDITION',\n",
    "    hue='PRIM_CONTRIBUTORY_CAUSE',\n",
    "    data=plot_data,\n",
    "    order=top_weather,\n",
    "    hue_order=top_cause_names\n",
    ")\n",
    "\n",
    "plt.title('Top Contributory Causes by Weather Condition', fontsize=16, pad=20)\n",
    "plt.xlabel('Weather Condition', fontsize=12)\n",
    "plt.ylabel('Number of Accidents', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Primary Cause', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How does lighting condition affect accident causation patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['LIGHTING_CONDITION'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['LIGHTING_CONDITION'] = data_df['LIGHTING_CONDITION'].replace({\n",
    "    'DARKNESS, LIGHTED ROAD': 'DARKNESS',\n",
    "   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_df = data_df[['LIGHTING_CONDITION', 'PRIM_CONTRIBUTORY_CAUSE']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_lighting = light_df['LIGHTING_CONDITION'].value_counts().nlargest(5).index\n",
    "top_causes = light_df['PRIM_CONTRIBUTORY_CAUSE'].value_counts().nlargest(5).index\n",
    "\n",
    "filtered_df = light_df[\n",
    "    light_df['LIGHTING_CONDITION'].isin(top_lighting) &\n",
    "    light_df['PRIM_CONTRIBUTORY_CAUSE'].isin(top_causes)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=filtered_df,\n",
    "              x='LIGHTING_CONDITION',\n",
    "              hue='PRIM_CONTRIBUTORY_CAUSE',\n",
    "              palette='Set3')\n",
    "\n",
    "plt.title('Top Accident Causes by Lighting Condition')\n",
    "plt.xlabel('Lighting Condition')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Primary Cause', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do driver demographics (age, gender) correlate with accident causes?\n",
    "\n",
    "##### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to include only what is in the top causes\n",
    "filtered_df = data_df[\n",
    "    data_df['PRIM_CONTRIBUTORY_CAUSE'].isin(top_causes)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Countplot\n",
    "sns.countplot(data=filtered_df, x='SEX', hue='PRIM_CONTRIBUTORY_CAUSE', palette='Set2')\n",
    "\n",
    "# Set title, labels \n",
    "plt.title('Crash Causes by Driver Gender')\n",
    "plt.xlabel('Driver Gender')\n",
    "plt.ylabel('Number of Crashes')\n",
    "\n",
    "# Legend\n",
    "plt.legend(title='Contributory Cause', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "Gender plays a significant role in road accidents, with males contributing to a much higher number of accidents for all listed causes.\n",
    "This may suggest that male drivers are either more frequently involved in risky driving behaviors or are more represented in the driving population (or both)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to remove age below 16 and above 100\n",
    "filtered_df = data_df[\n",
    "    (data_df['AGE'] >= 16) & \n",
    "    (data_df['AGE'] <= 100) & \n",
    "    (data_df['PRIM_CONTRIBUTORY_CAUSE'].isin(top_causes))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Boxplot\n",
    "sns.boxplot(data=filtered_df, x='PRIM_CONTRIBUTORY_CAUSE', y='AGE', palette='pastel')\n",
    "\n",
    "# Set title, labels \n",
    "plt.title('Driver Age Distribution by Crash Cause')\n",
    "plt.xlabel('Contributory Cause')\n",
    "plt.ylabel('Driver Age')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "The data distributions for each category are similar in terms of center and spread.\n",
    "\n",
    "All groups contain significant outliers, which might warrant further investigation.\n",
    "\n",
    "No category stands out as significantly different in terms of overall distribution, but the green category shows more variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting target and features from the dataframe\n",
    "\n",
    "# Features\n",
    "X = data_df.drop(['PRIM_CONTRIBUTORY_CAUSE', 'CRASH_DATE'], axis=1)  \n",
    "# Target\n",
    "y = data_df['PRIM_CONTRIBUTORY_CAUSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "#Categorical columns\n",
    "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing\n",
    "A preprocessing pipeline was created, where numeric columns underwent median imputation and standardization (StandardScaler), while categorical columns were processed using imputation and one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create transformers\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    \n",
    "categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression\n",
    "Logistic regression model was used since we're predicting categories (causes of accidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Pipeline\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        max_iter=1000, \n",
    "        class_weight='balanced',\n",
    "        solver='saga',\n",
    "        random_state=42))\n",
    "])\n",
    "\n",
    "# Keep only rows where the target is in top causes\n",
    "mask = y_train.isin(top_causes)\n",
    "\n",
    "X_train_top = X_train[mask]\n",
    "y_train_top = y_train[mask]\n",
    "\n",
    "# Apply same filter to test data\n",
    "mask_test = y_test.isin(top_causes)\n",
    "X_test_top = X_test[mask_test]\n",
    "y_test_top = y_test[mask_test]\n",
    "\n",
    "\n",
    "# Model training\n",
    "lr_pipeline.fit(X_train_top, y_train_top)\n",
    "y_pred_top = lr_pipeline.predict(X_test_top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test_top, y_pred_top)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_top, y_pred_top))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "The model achieved an overall accuracy of 27.9%. This indicates the model is struggling to accurately classify the primary contributory causes of crashes.\n",
    "Precision and recall are generally low, suggesting the model has difficulty distinguishing between accident causes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_top, y_pred_top, labels=np.unique(y_test_top))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=y_test_top.unique(), yticklabels=y_test_top.unique(), cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "The confusion matrix suggests that while the model has reasonable accuracy, there is room for improvement, especially in reducing confusion between certain classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier\n",
    "\n",
    "While logistic regression served as a reasonable baseline, its performance on this multi-class classification problem proved limited. To enhance predictive capability, we implemented a Random Forest classifier, which typically handles complex decision boundaries and multi-class scenarios more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "rf_pipeline.fit(X_train_top, y_train_top)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = rf_pipeline.predict(X_test_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test_top, y_pred_rf))\n",
    "\n",
    "# Report\n",
    "print(classification_report(y_test_top, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "The Random Forest model achieved an accuracy of 34.1%, which is an improvement over the logistic regression baseline (27.9%).\n",
    "* \"Failing to Yield Right-of-Way\" had the highest precision (42%) and recall (50%), showing the model was most confident and accurate with this class.\n",
    "* Classes like \"Improper Lane Usage\" and \"Improper Overtaking/Passing\" had low precision and recall, indicating the model struggles to distinguish these.\n",
    "\n",
    "The model still shows difficulty predicting minority or more ambiguous causes of crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_top, y_pred_rf, labels=np.unique(y_test_top))\n",
    "\n",
    "# Plot figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=y_test_top.unique(), yticklabels=y_test_top.unique(), cmap='YlGnBu')\n",
    "\n",
    "# Set title, labels\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - Random Forest\")\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "* \"improper overtaking/passing\" has the highest correct predictions (3,376), likely due to its larger representation in the data (class imbalance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "\n",
    "Hyperparameter tuning was done using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'classifier__C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'classifier__solver': ['saga', 'lbfgs'],  # Solvers for multiclass\n",
    "    'classifier__penalty': ['l2'],  # L1 requires sparse data & saga only\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    lr_pipeline,\n",
    "    param_grid,\n",
    "    scoring='f1_weighted',  # Or 'accuracy', 'recall_macro' etc.\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_top, y_train_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict with best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_top)  # Use X_test_top to match your filtered causes\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\"Tuned Model Accuracy:\", accuracy_score(y_test_top, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_top, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "The best logistic regression parameters were:C=10, penalty='l2', and solver='lbfgs'.\n",
    "The overall accuracy after tuning was ~27.8%, indicating only a slight improvement from the untuned mode.\n",
    "\n",
    "The logistic regression model, even after tuning, has limited predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the Random Forest hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Param grid\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [10, 20, None],\n",
    "    'classifier__min_samples_split': [2, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2],\n",
    "}\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search_rf = GridSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring='f1_weighted',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV\n",
    "grid_search_rf.fit(X_train_top, y_train_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print(\"Best Parameters:\", grid_search_rf.best_params_)\n",
    "\n",
    "# Predict using best model\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "y_pred_rf_tuned = best_rf_model.predict(X_test_top)\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\"Tuned Random Forest Accuracy:\", accuracy_score(y_test_top, y_pred_rf_tuned))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_top, y_pred_rf_tuned))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "While the accuracy dropped slightly from 34.1% to 33.3%, the weighted F1-score remained strong, suggesting the model now treats minor classes more equitably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feature importances from the model\n",
    "importances = rf_pipeline.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Feature names\n",
    "ohe = rf_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']\n",
    "categorical_feature_names = ohe.get_feature_names_out(categorical_cols)\n",
    "all_feature_names = list(numeric_cols) + list(categorical_feature_names)\n",
    "\n",
    "# Create a DataFrame\n",
    "feat_imp_df = pd.DataFrame({'feature': all_feature_names, 'importance': importances})\n",
    "feat_imp_df = feat_imp_df.sort_values(by='importance', ascending=False).head(20)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feat_imp_df['feature'], feat_imp_df['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Feature Importances (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "* 'AGE' and 'VEHICLE_YEAR' are by far the most important features.\n",
    "* Gender (SEX_M/F) has minimal influence, suggesting it’s not a major predictor.\n",
    "* Vehicle makes show slight variations in importance, possibly reflecting differences in crash rates by brand.\n",
    "* Weather/lighting conditions  have negligible effects, implying they may not significantly impact predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project focused on predicting the **Primary Contributory Cause** of vehicle crashes using traffic accident data from Chicago. The workflow involved data preprocessing, exploratory analysis, model building, and hyperparameter tuning using Logistic Regression and Random Forest classifiers.\n",
    "\n",
    "- The **Top 5 accident causes** were:\n",
    "  - *Failing to Yield Right-of-Way*\n",
    "  - *Following Too Closely*\n",
    "  - *Improper Lane Usage*\n",
    "  - *Failing to Reduce Speed to Avoid Crash*\n",
    "  - *Improper Overtaking/Passing*\n",
    "\n",
    "- **Model Performance**\n",
    "\n",
    "| Model                | Accuracy | Key Notes |\n",
    "|---------------------|----------|-----------|\n",
    "| Logistic Regression (Tuned) | 27.8%   | Better recall for certain causes but low precision overall |\n",
    "| Random Forest (Tuned)       | 33.3%   | Better balance across classes, but slight accuracy drop after tuning |\n",
    "\n",
    "- Despite tuning, accuracy remained modest due to:\n",
    "  - Class imbalance\n",
    "  - Overlapping feature characteristics\n",
    "  - Limited discriminative power of available features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "**Stakeholders**\n",
    "* Driver-Focused Programs: Develop public awareness and training campaigns addressing top causes.\n",
    "* Targeted interventions; Focus enforcement and road safety improvements on road signage.\n",
    "\n",
    "**Data Team**\n",
    "- Handle class imbalance using other techniques\n",
    "- Try advanced models like **XGBoost** or **LightGBM**\n",
    "- Evaluate using additional metrics (e.g., ROC AUC, confusion matrices)\n",
    "\n",
    "While prediction accuracy is moderate, this project highlights how **machine learning** can uncover valuable **patterns in accident causation**, providing data-driven insights for **policy makers, road safety advocates**, and **urban planners** to mitigate crash risks and improve public safety."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
